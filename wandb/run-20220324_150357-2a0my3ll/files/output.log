<xml><var name="Accumulator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Accumulator%27&gt;" isContainer="True" />
<var name="Animator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Animator%27&gt;" isContainer="True" />
<var name="DataFrame" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.frame.DataFrame%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="41" isIPythonHidden="True" />
<var name="MultiIndex" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.indexes.multi.MultiIndex%27&gt;" isContainer="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="W" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29" isContainer="True" shape="(784, 10)" />
<var name="X" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.6164%2C  0.1436%2C -0.4461%2C -0.0826%2C  0.3884%5D%2C%0A        %5B 1.4740%2C  0.1372%2C -0.7849%2C -0.4987%2C -0.2021%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="X_prob" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="_" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_10" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_13" type="float" qualifier="builtins" value="0.0673" isIPythonHidden="True" />
<var name="_2" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_20" type="float" qualifier="builtins" value="0.8367" isIPythonHidden="True" />
<var name="_22" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_23" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_25" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_27" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_28" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_3" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_30" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_33" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="_39" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_5" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_7" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_8" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="__" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="___" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5B%27E%3A%5C%5CAI_project%27%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import torch%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="import torch%0Aimport wandb%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Awandb.init%28project=%22my-test-project%22%2C entity=%22j1feng%22%29%0Awandb.config = %7B%0A    %22learning_rate%22%3A 0.1%2C%0A    %22epochs%22%3A 10%2C%0A    %22batch_size%22%3A 256%0A%7D%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i25" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i26" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i27" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i28" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i29" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i30" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i31" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i32" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i33" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i34" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A  %23 %E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6net %E6%98%AFtorch.nn.Module%E7%B1%BB%E5%9E%8B%28%E6%88%96%E8%80%85%E5%85%B6%E5%AD%90%E7%B1%BB%29%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i35" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i36" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i37" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i38" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i39" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i40" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="41" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers%5C%5Cpydev%5C%5C_pydevd_bundle%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="accuracy" type="function" qualifier="builtins" value="%3Cfunction accuracy at 0x000002A8C88E74C0&gt;" isContainer="True" />
<var name="b" type="Tensor" qualifier="torch" value="tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29" isContainer="True" shape="(10,)" />
<var name="batch_size" type="int" qualifier="builtins" value="256" />
<var name="cross_entropy" type="function" qualifier="builtins" value="%3Cfunction cross_entropy at 0x000002A8CABD3430&gt;" isContainer="True" />
<var name="d2l" type="module" qualifier="builtins" value="%3Cmodule %27d2l.torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cd2l%5C%5Ctorch.py%27&gt;" isContainer="True" />
<var name="display" type="module" qualifier="builtins" value="%3Cmodule %27IPython.display%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5CIPython%5C%5Cdisplay.py%27&gt;" isContainer="True" />
<var name="evaluate_accuracy" type="function" qualifier="builtins" value="%3Cfunction evaluate_accuracy at 0x000002A8CABDCE50&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x000002A8AAE88F40&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="lr" type="float" qualifier="builtins" value="0.1" />
<var name="net" type="function" qualifier="builtins" value="%3Cfunction net at 0x000002A8CABD39D0&gt;" isContainer="True" />
<var name="num_epochs" type="int" qualifier="builtins" value="10" />
<var name="num_inputs" type="int" qualifier="builtins" value="784" />
<var name="num_outputs" type="int" qualifier="builtins" value="10" />
<var name="print_columns" type="function" qualifier="builtins" value="%3Cfunction print_columns at 0x000002A8C91C08B0&gt;" isContainer="True" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers-pro%5C%5Cjupyter_debug%5C%5Cpydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x000002A8C883F9D0&gt;" isContainer="True" />
<var name="softmax" type="function" qualifier="builtins" value="%3Cfunction softmax at 0x000002A8CABDCEE0&gt;" isContainer="True" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="test_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CABD58B0&gt;" isContainer="True" shape="40" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Ctorch%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="train_ch3" type="function" qualifier="builtins" value="%3Cfunction train_ch3 at 0x000002A8CABB9DC0&gt;" isContainer="True" />
<var name="train_epoch_ch3" type="function" qualifier="builtins" value="%3Cfunction train_epoch_ch3 at 0x000002A8CABD3280&gt;" isContainer="True" />
<var name="train_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CAC0F9D0&gt;" isContainer="True" shape="235" />
<var name="updater" type="function" qualifier="builtins" value="%3Cfunction updater at 0x000002A8C919E430&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cwandb%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="y" type="Tensor" qualifier="torch" value="tensor%28%5B0%2C 2%5D%29" isContainer="True" shape="(2,)" />
<var name="y_hat" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1000%2C 0.3000%2C 0.6000%5D%2C%0A        %5B0.3000%2C 0.2000%2C 0.5000%5D%5D%29" isContainer="True" shape="(2, 3)" />
</xml>
<xml><var name="Accumulator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Accumulator%27&gt;" isContainer="True" />
<var name="Animator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Animator%27&gt;" isContainer="True" />
<var name="DataFrame" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.frame.DataFrame%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="42" isIPythonHidden="True" />
<var name="MultiIndex" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.indexes.multi.MultiIndex%27&gt;" isContainer="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="W" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29" isContainer="True" shape="(784, 10)" />
<var name="X" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.6164%2C  0.1436%2C -0.4461%2C -0.0826%2C  0.3884%5D%2C%0A        %5B 1.4740%2C  0.1372%2C -0.7849%2C -0.4987%2C -0.2021%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="X_prob" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="_" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_10" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_13" type="float" qualifier="builtins" value="0.0673" isIPythonHidden="True" />
<var name="_2" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_20" type="float" qualifier="builtins" value="0.8367" isIPythonHidden="True" />
<var name="_22" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_23" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_25" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_27" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_28" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_3" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_30" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_33" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="_39" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_5" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_7" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_8" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="__" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="___" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5B%27E%3A%5C%5CAI_project%27%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import torch%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="import torch%0Aimport wandb%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Awandb.init%28project=%22my-test-project%22%2C entity=%22j1feng%22%29%0Awandb.config = %7B%0A    %22learning_rate%22%3A 0.1%2C%0A    %22epochs%22%3A 10%2C%0A    %22batch_size%22%3A 256%0A%7D%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i25" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i26" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i27" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i28" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i29" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i30" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i31" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i32" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i33" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i34" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A  %23 %E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6net %E6%98%AFtorch.nn.Module%E7%B1%BB%E5%9E%8B%28%E6%88%96%E8%80%85%E5%85%B6%E5%AD%90%E7%B1%BB%29%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i35" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i36" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i37" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i38" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i39" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i40" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i41" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="42" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers%5C%5Cpydev%5C%5C_pydevd_bundle%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="accuracy" type="function" qualifier="builtins" value="%3Cfunction accuracy at 0x000002A8C88E74C0&gt;" isContainer="True" />
<var name="b" type="Tensor" qualifier="torch" value="tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29" isContainer="True" shape="(10,)" />
<var name="batch_size" type="int" qualifier="builtins" value="256" />
<var name="cross_entropy" type="function" qualifier="builtins" value="%3Cfunction cross_entropy at 0x000002A8CABD3430&gt;" isContainer="True" />
<var name="d2l" type="module" qualifier="builtins" value="%3Cmodule %27d2l.torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cd2l%5C%5Ctorch.py%27&gt;" isContainer="True" />
<var name="display" type="module" qualifier="builtins" value="%3Cmodule %27IPython.display%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5CIPython%5C%5Cdisplay.py%27&gt;" isContainer="True" />
<var name="evaluate_accuracy" type="function" qualifier="builtins" value="%3Cfunction evaluate_accuracy at 0x000002A8CABDCE50&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x000002A8AAE88F40&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="lr" type="float" qualifier="builtins" value="0.1" />
<var name="net" type="function" qualifier="builtins" value="%3Cfunction net at 0x000002A8CABD39D0&gt;" isContainer="True" />
<var name="num_epochs" type="int" qualifier="builtins" value="10" />
<var name="num_inputs" type="int" qualifier="builtins" value="784" />
<var name="num_outputs" type="int" qualifier="builtins" value="10" />
<var name="print_columns" type="function" qualifier="builtins" value="%3Cfunction print_columns at 0x000002A8CDBF9DC0&gt;" isContainer="True" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers-pro%5C%5Cjupyter_debug%5C%5Cpydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x000002A8C883F9D0&gt;" isContainer="True" />
<var name="softmax" type="function" qualifier="builtins" value="%3Cfunction softmax at 0x000002A8CABDCEE0&gt;" isContainer="True" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="test_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CABD58B0&gt;" isContainer="True" shape="40" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Ctorch%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="train_ch3" type="function" qualifier="builtins" value="%3Cfunction train_ch3 at 0x000002A8CABB9DC0&gt;" isContainer="True" />
<var name="train_epoch_ch3" type="function" qualifier="builtins" value="%3Cfunction train_epoch_ch3 at 0x000002A8CABD3280&gt;" isContainer="True" />
<var name="train_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CAC0F9D0&gt;" isContainer="True" shape="235" />
<var name="updater" type="function" qualifier="builtins" value="%3Cfunction updater at 0x000002A8C919E430&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cwandb%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="y" type="Tensor" qualifier="torch" value="tensor%28%5B0%2C 2%5D%29" isContainer="True" shape="(2,)" />
<var name="y_hat" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1000%2C 0.3000%2C 0.6000%5D%2C%0A        %5B0.3000%2C 0.2000%2C 0.5000%5D%5D%29" isContainer="True" shape="(2, 3)" />
</xml>
<xml><var name="Accumulator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Accumulator%27&gt;" isContainer="True" />
<var name="Animator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Animator%27&gt;" isContainer="True" />
<var name="DataFrame" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.frame.DataFrame%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="43" isIPythonHidden="True" />
<var name="MultiIndex" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.indexes.multi.MultiIndex%27&gt;" isContainer="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="W" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29" isContainer="True" shape="(784, 10)" />
<var name="X" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.6164%2C  0.1436%2C -0.4461%2C -0.0826%2C  0.3884%5D%2C%0A        %5B 1.4740%2C  0.1372%2C -0.7849%2C -0.4987%2C -0.2021%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="X_prob" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="_" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_10" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_13" type="float" qualifier="builtins" value="0.0673" isIPythonHidden="True" />
<var name="_2" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_20" type="float" qualifier="builtins" value="0.8367" isIPythonHidden="True" />
<var name="_22" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_23" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_25" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_27" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_28" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_3" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_30" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_33" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="_39" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_5" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_7" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_8" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="__" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="___" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5B%27E%3A%5C%5CAI_project%27%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import torch%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="import torch%0Aimport wandb%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Awandb.init%28project=%22my-test-project%22%2C entity=%22j1feng%22%29%0Awandb.config = %7B%0A    %22learning_rate%22%3A 0.1%2C%0A    %22epochs%22%3A 10%2C%0A    %22batch_size%22%3A 256%0A%7D%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i25" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i26" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i27" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i28" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i29" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i30" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i31" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i32" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i33" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i34" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A  %23 %E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6net %E6%98%AFtorch.nn.Module%E7%B1%BB%E5%9E%8B%28%E6%88%96%E8%80%85%E5%85%B6%E5%AD%90%E7%B1%BB%29%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i35" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i36" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i37" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i38" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i39" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i40" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i41" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i42" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.model%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="43" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers%5C%5Cpydev%5C%5C_pydevd_bundle%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="accuracy" type="function" qualifier="builtins" value="%3Cfunction accuracy at 0x000002A8C88E74C0&gt;" isContainer="True" />
<var name="b" type="Tensor" qualifier="torch" value="tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29" isContainer="True" shape="(10,)" />
<var name="batch_size" type="int" qualifier="builtins" value="256" />
<var name="cross_entropy" type="function" qualifier="builtins" value="%3Cfunction cross_entropy at 0x000002A8CABD3430&gt;" isContainer="True" />
<var name="d2l" type="module" qualifier="builtins" value="%3Cmodule %27d2l.torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cd2l%5C%5Ctorch.py%27&gt;" isContainer="True" />
<var name="display" type="module" qualifier="builtins" value="%3Cmodule %27IPython.display%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5CIPython%5C%5Cdisplay.py%27&gt;" isContainer="True" />
<var name="evaluate_accuracy" type="function" qualifier="builtins" value="%3Cfunction evaluate_accuracy at 0x000002A8CABDCE50&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x000002A8AAE88F40&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="lr" type="float" qualifier="builtins" value="0.1" />
<var name="net" type="function" qualifier="builtins" value="%3Cfunction net at 0x000002A8CABD39D0&gt;" isContainer="True" />
<var name="num_epochs" type="int" qualifier="builtins" value="10" />
<var name="num_inputs" type="int" qualifier="builtins" value="784" />
<var name="num_outputs" type="int" qualifier="builtins" value="10" />
<var name="print_columns" type="function" qualifier="builtins" value="%3Cfunction print_columns at 0x000002A8CDAD71F0&gt;" isContainer="True" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers-pro%5C%5Cjupyter_debug%5C%5Cpydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x000002A8C883F9D0&gt;" isContainer="True" />
<var name="softmax" type="function" qualifier="builtins" value="%3Cfunction softmax at 0x000002A8CABDCEE0&gt;" isContainer="True" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="test_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CABD58B0&gt;" isContainer="True" shape="40" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Ctorch%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="train_ch3" type="function" qualifier="builtins" value="%3Cfunction train_ch3 at 0x000002A8CABB9DC0&gt;" isContainer="True" />
<var name="train_epoch_ch3" type="function" qualifier="builtins" value="%3Cfunction train_epoch_ch3 at 0x000002A8CABD3280&gt;" isContainer="True" />
<var name="train_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CAC0F9D0&gt;" isContainer="True" shape="235" />
<var name="updater" type="function" qualifier="builtins" value="%3Cfunction updater at 0x000002A8C919E430&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cwandb%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="y" type="Tensor" qualifier="torch" value="tensor%28%5B0%2C 2%5D%29" isContainer="True" shape="(2,)" />
<var name="y_hat" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1000%2C 0.3000%2C 0.6000%5D%2C%0A        %5B0.3000%2C 0.2000%2C 0.5000%5D%5D%29" isContainer="True" shape="(2, 3)" />
</xml>
<xml><var name="Accumulator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Accumulator%27&gt;" isContainer="True" />
<var name="Animator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Animator%27&gt;" isContainer="True" />
<var name="DataFrame" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.frame.DataFrame%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="44" isIPythonHidden="True" />
<var name="MultiIndex" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.indexes.multi.MultiIndex%27&gt;" isContainer="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="W" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29" isContainer="True" shape="(784, 10)" />
<var name="X" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.6164%2C  0.1436%2C -0.4461%2C -0.0826%2C  0.3884%5D%2C%0A        %5B 1.4740%2C  0.1372%2C -0.7849%2C -0.4987%2C -0.2021%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="X_prob" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="_" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_10" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_13" type="float" qualifier="builtins" value="0.0673" isIPythonHidden="True" />
<var name="_2" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_20" type="float" qualifier="builtins" value="0.8367" isIPythonHidden="True" />
<var name="_22" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_23" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_25" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_27" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_28" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_3" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_30" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_33" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="_39" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_5" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_7" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_8" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="__" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="___" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5B%27E%3A%5C%5CAI_project%27%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.model%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import torch%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="import torch%0Aimport wandb%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Awandb.init%28project=%22my-test-project%22%2C entity=%22j1feng%22%29%0Awandb.config = %7B%0A    %22learning_rate%22%3A 0.1%2C%0A    %22epochs%22%3A 10%2C%0A    %22batch_size%22%3A 256%0A%7D%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i25" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i26" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i27" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i28" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i29" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i30" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i31" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i32" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i33" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i34" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A  %23 %E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6net %E6%98%AFtorch.nn.Module%E7%B1%BB%E5%9E%8B%28%E6%88%96%E8%80%85%E5%85%B6%E5%AD%90%E7%B1%BB%29%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i35" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i36" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i37" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i38" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i39" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i40" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i41" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i42" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.model%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i43" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.Module%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="44" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers%5C%5Cpydev%5C%5C_pydevd_bundle%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="accuracy" type="function" qualifier="builtins" value="%3Cfunction accuracy at 0x000002A8C88E74C0&gt;" isContainer="True" />
<var name="b" type="Tensor" qualifier="torch" value="tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29" isContainer="True" shape="(10,)" />
<var name="batch_size" type="int" qualifier="builtins" value="256" />
<var name="cross_entropy" type="function" qualifier="builtins" value="%3Cfunction cross_entropy at 0x000002A8CABD3430&gt;" isContainer="True" />
<var name="d2l" type="module" qualifier="builtins" value="%3Cmodule %27d2l.torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cd2l%5C%5Ctorch.py%27&gt;" isContainer="True" />
<var name="display" type="module" qualifier="builtins" value="%3Cmodule %27IPython.display%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5CIPython%5C%5Cdisplay.py%27&gt;" isContainer="True" />
<var name="evaluate_accuracy" type="function" qualifier="builtins" value="%3Cfunction evaluate_accuracy at 0x000002A8CABDCE50&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x000002A8AAE88F40&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="lr" type="float" qualifier="builtins" value="0.1" />
<var name="net" type="function" qualifier="builtins" value="%3Cfunction net at 0x000002A8CABD39D0&gt;" isContainer="True" />
<var name="num_epochs" type="int" qualifier="builtins" value="10" />
<var name="num_inputs" type="int" qualifier="builtins" value="784" />
<var name="num_outputs" type="int" qualifier="builtins" value="10" />
<var name="print_columns" type="function" qualifier="builtins" value="%3Cfunction print_columns at 0x000002A8CDBF99D0&gt;" isContainer="True" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers-pro%5C%5Cjupyter_debug%5C%5Cpydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x000002A8C883F9D0&gt;" isContainer="True" />
<var name="softmax" type="function" qualifier="builtins" value="%3Cfunction softmax at 0x000002A8CABDCEE0&gt;" isContainer="True" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="test_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CABD58B0&gt;" isContainer="True" shape="40" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Ctorch%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="train_ch3" type="function" qualifier="builtins" value="%3Cfunction train_ch3 at 0x000002A8CABB9DC0&gt;" isContainer="True" />
<var name="train_epoch_ch3" type="function" qualifier="builtins" value="%3Cfunction train_epoch_ch3 at 0x000002A8CABD3280&gt;" isContainer="True" />
<var name="train_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CAC0F9D0&gt;" isContainer="True" shape="235" />
<var name="updater" type="function" qualifier="builtins" value="%3Cfunction updater at 0x000002A8C919E430&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cwandb%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="y" type="Tensor" qualifier="torch" value="tensor%28%5B0%2C 2%5D%29" isContainer="True" shape="(2,)" />
<var name="y_hat" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1000%2C 0.3000%2C 0.6000%5D%2C%0A        %5B0.3000%2C 0.2000%2C 0.5000%5D%5D%29" isContainer="True" shape="(2, 3)" />
</xml>
<xml><var name="Accumulator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Accumulator%27&gt;" isContainer="True" />
<var name="Animator" type="type" qualifier="builtins" value="%3Cclass %27__main__.Animator%27&gt;" isContainer="True" />
<var name="DataFrame" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.frame.DataFrame%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="45" isIPythonHidden="True" />
<var name="MultiIndex" type="type" qualifier="builtins" value="%3Cclass %27pandas.core.indexes.multi.MultiIndex%27&gt;" isContainer="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="W" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29" isContainer="True" shape="(784, 10)" />
<var name="X" type="Tensor" qualifier="torch" value="tensor%28%5B%5B-0.6164%2C  0.1436%2C -0.4461%2C -0.0826%2C  0.3884%5D%2C%0A        %5B 1.4740%2C  0.1372%2C -0.7849%2C -0.4987%2C -0.2021%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="X_prob" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29" isContainer="True" shape="(2, 5)" />
<var name="_" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_10" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_13" type="float" qualifier="builtins" value="0.0673" isIPythonHidden="True" />
<var name="_2" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_20" type="float" qualifier="builtins" value="0.8367" isIPythonHidden="True" />
<var name="_22" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        %5B-0.0058%2C  0.0116%2C  0.0186%2C  ...%2C -0.0101%2C  0.0005%2C -0.0051%5D%2C%0A        ...%2C%0A        %5B-0.0270%2C  0.0039%2C  0.0573%2C  ...%2C -0.0126%2C -0.0433%2C -0.0150%5D%2C%0A        %5B-0.0251%2C -0.0110%2C  0.0123%2C  ...%2C -0.0024%2C -0.0224%2C -0.0192%5D%2C%0A        %5B 0.0052%2C  0.0022%2C  0.0108%2C  ...%2C  0.0099%2C -0.0128%2C -0.0044%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_23" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_25" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1141%2C 0.2441%2C 0.1353%2C 0.1947%2C 0.3118%5D%2C%0A        %5B0.5906%2C 0.1551%2C 0.0617%2C 0.0821%2C 0.1105%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_27" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_28" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_3" type="Tensor" qualifier="torch" value="tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29" isContainer="True" shape="(2, 1)" isIPythonHidden="True" />
<var name="_30" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="_33" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="_39" type="float" qualifier="builtins" value="0.8321" isIPythonHidden="True" />
<var name="_5" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_7" type="Tensor" qualifier="torch" value="tensor%28%5B0.1000%2C 0.5000%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="_8" type="Tensor" qualifier="torch" value="tensor%28%5B2.3026%2C 0.6931%5D%29" isContainer="True" shape="(2,)" isIPythonHidden="True" />
<var name="__" type="float" qualifier="builtins" value="0.0907" isIPythonHidden="True" />
<var name="___" type="float" qualifier="builtins" value="0.5" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5B%27E%3A%5C%5CAI_project%27%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.Module%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import torch%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="import torch%0Aimport wandb%0Afrom IPython import display%0Afrom d2l import torch as d2l%0A%0Awandb.init%28project=%22my-test-project%22%2C entity=%22j1feng%22%29%0Awandb.config = %7B%0A    %22learning_rate%22%3A 0.1%2C%0A    %22epochs%22%3A 10%2C%0A    %22batch_size%22%3A 256%0A%7D%0Abatch_size = 256%0Atrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="num_inputs = 784%0Anum_outputs = 10%0A%0AW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%0Ab = torch.zeros%28num_outputs%2C requires_grad=True%29%0AW%2C b" isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i25" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i26" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i27" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i28" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i29" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%0AX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%0AX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C" isIPythonHidden="True" />
<var name="_i30" type="str" qualifier="builtins" value="accuracy%28y_hat%2C y%29 / len%28y%29" isIPythonHidden="True" />
<var name="_i31" type="str" qualifier="builtins" value="def evaluate_accuracy%28net%2C data_iter%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E5%9C%A8%E6%8C%87%E5%AE%9A%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%B2%BE%E5%BA%A6%22%22%22%0A    if isinstance%28net%2C torch.nn.Module%29%3A%0A        net.eval%28%29  %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AF%84%E4%BC%B0%E6%A8%A1%E5%BC%8F%0A    metric = Accumulator%282%29%0A    with torch.no_grad%28%29%3A%0A        for X%2C y in data_iter%3A%0A            metric.add%28accuracy%28net%28X%29%2C y%29%2C y.numel%28%29%29%0A    return metric%5B0%5D / metric%5B1%5D" isIPythonHidden="True" />
<var name="_i32" type="str" qualifier="builtins" value="class Accumulator%3A%0A    %22%22%22%E5%9C%A8n%E4%B8%AA%E5%8F%98%E9%87%8F%E4%B8%8A%E7%B4%AF%E5%8A%A0%22%22%22%0A%0A    def __init__%28self%2C n%29%3A%0A        self.data = %5B0%2C 0%5D %2A n%0A%0A    def add%28self%2C %2Aargs%29%3A%0A        self.data = %5Ba %2B float%28b%29 for a%2C b in zip%28self.data%2C args%29%5D%0A%0A    def reset%28self%29%3A%0A        self.data = %5B0%2C 0%5D %2A len%28self.data%29%0A%0A    def __getitem__%28self%2C idx%29%3A%0A        return self.data%5Bidx%5D" isIPythonHidden="True" />
<var name="_i33" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i34" type="str" qualifier="builtins" value="def train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E4%B8%80%E4%B8%AA%E8%BF%AD%E4%BB%A3%E5%91%A8%E6%9C%9F%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    %23 %E5%B0%86%E6%A8%A1%E5%9E%8B%E8%AE%BE%E7%BD%AE%E4%B8%BA%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%BC%8F%0A    if isinstance%28net%2C torch.nn.Module%29%3A  %23 %E5%88%A4%E6%96%AD%E6%98%AF%E5%90%A6net %E6%98%AFtorch.nn.Module%E7%B1%BB%E5%9E%8B%28%E6%88%96%E8%80%85%E5%85%B6%E5%AD%90%E7%B1%BB%29%0A        net.train%28%29%0A    %23 %E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E6%80%BB%E5%92%8C%E3%80%81%E8%AE%AD%E7%BB%83%E5%87%86%E7%A1%AE%E5%BA%A6%E6%80%BB%E5%92%8C%E3%80%81%E6%A0%B7%E6%9C%AC%E6%95%B0%0A    metric = Accumulator%283%29%0A    for X%2C y in train_iter%3A%0A        %23 %E8%AE%A1%E7%AE%97%E6%A2%AF%E5%BA%A6%E5%B9%B6%E6%9B%B4%E6%96%B0%E5%8F%82%E6%95%B0%0A        y_hat = net%28X%29%0A        l = loss%28y_hat%2C y%29%0A        if isinstance%28updater%2C torch.optim.Optimizer%29%3A%0A            %23 %E4%BD%BF%E7%94%A8PyTorch%E5%86%85%E7%BD%AE%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            updater.zero_grad%28%29%0A            l.mean%28%29.backward%28%29%0A            updater.step%28%29%0A        else%3A%0A            %23 %E4%BD%BF%E7%94%A8%E5%AE%9A%E5%88%B6%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%0A            l.sum%28%29.backward%28%29%0A            updater%28X.shape%5B0%5D%29%0A        metric.add%28float%28l.sum%28%29%29%2C accuracy%28y_hat%2C y%29%2C y.numel%28%29%29%0A    %23 %E8%BF%94%E5%9B%9E%E8%AE%AD%E7%BB%83%E6%8D%9F%E5%A4%B1%E5%92%8C%E8%AE%AD%E7%BB%83%E7%B2%BE%E5%BA%A6%0A    return metric%5B0%5D / metric%5B2%5D%2C metric%5B1%5D / metric%5B2%5D" isIPythonHidden="True" />
<var name="_i35" type="str" qualifier="builtins" value="class Animator%3A  %23%40save%0A    %22%22%22%E5%9C%A8%E5%8A%A8%E7%94%BB%E4%B8%AD%E7%BB%98%E5%88%B6%E6%95%B0%E6%8D%AE%22%22%22%0A%0A    def __init__%28self%2C xlabel=None%2C ylabel=None%2C legend=None%2C xlim=None%2C%0A                 ylim=None%2C xscale=%27linear%27%2C yscale=%27linear%27%2C%0A                 fmts=%28%27-%27%2C %27m--%27%2C %27g-.%27%2C %27r%3A%27%29%2C nrows=1%2C ncols=1%2C%0A                 figsize=%283.5%2C 2.5%29%29%3A%0A        %23 %E5%A2%9E%E9%87%8F%E5%9C%B0%E7%BB%98%E5%88%B6%E5%A4%9A%E6%9D%A1%E7%BA%BF%0A        if legend is None%3A%0A            legend = %5B%5D%0A        d2l.use_svg_display%28%29%0A        self.fig%2C self.axes = d2l.plt.subplots%28nrows%2C ncols%2C figsize=figsize%29%0A        if nrows %2A ncols == 1%3A%0A            self.axes = %5Bself.axes%2C %5D%0A        %23 %E4%BD%BF%E7%94%A8lambda%E5%87%BD%E6%95%B0%E6%8D%95%E8%8E%B7%E5%8F%82%E6%95%B0%0A        self.config_axes = lambda%3A d2l.set_axes%28%0A            self.axes%5B0%5D%2C xlabel%2C ylabel%2C xlim%2C ylim%2C xscale%2C yscale%2C legend%29%0A        self.X%2C self.Y%2C self.fmts = None%2C None%2C fmts%0A%0A    def add%28self%2C x%2C y%29%3A%0A        %23 %E5%90%91%E5%9B%BE%E8%A1%A8%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%A4%9A%E4%B8%AA%E6%95%B0%E6%8D%AE%E7%82%B9%0A        if not hasattr%28y%2C %22__len__%22%29%3A%0A            y = %5By%5D%0A        n = len%28y%29%0A        if not hasattr%28x%2C %22__len__%22%29%3A%0A            x = %5Bx%5D %2A n%0A        if not self.X%3A%0A            self.X = %5B%5B%5D for _ in range%28n%29%5D%0A        if..." isIPythonHidden="True" />
<var name="_i36" type="str" qualifier="builtins" value="def train_ch3%28net%2C train_iter%2C test_iter%2C loss%2C num_epochs%2C updater%29%3A  %23%40save%0A    %22%22%22%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%EF%BC%88%E5%AE%9A%E4%B9%89%E8%A7%81%E7%AC%AC3%E7%AB%A0%EF%BC%89%22%22%22%0A    animator = Animator%28xlabel=%27epoch%27%2C xlim=%5B1%2C num_epochs%5D%2C ylim=%5B0.3%2C 0.9%5D%2C%0A                        legend=%5B%27train loss%27%2C %27train acc%27%2C %27test acc%27%5D%29%0A    for epoch in range%28num_epochs%29%3A%0A        train_metrics = train_epoch_ch3%28net%2C train_iter%2C loss%2C updater%29%0A        test_acc = evaluate_accuracy%28net%2C test_iter%29%0A        animator.add%28epoch %2B 1%2C train_metrics %2B %28test_acc%2C%29%29%0A    train_loss%2C train_acc = train_metrics%0A    assert train_loss %3C 0.5%2C train_loss%0A    assert train_acc %3C= 1 and train_acc &gt; 0.7%2C train_acc%0A    assert test_acc %3C= 1 and test_acc &gt; 0.7%2C test_acc" isIPythonHidden="True" />
<var name="_i37" type="str" qualifier="builtins" value="lr = 0.1%0A%0A%0Adef updater%28batch_size%29%3A%0A    return d2l.sgd%28%5BW%2C b%5D%2C lr%2C batch_size%29" isIPythonHidden="True" />
<var name="_i38" type="str" qualifier="builtins" value="num_epochs = 10%0Atrain_ch3%28net%2C train_iter%2C test_iter%2C cross_entropy%2C num_epochs%2C updater%29" isIPythonHidden="True" />
<var name="_i39" type="str" qualifier="builtins" value="evaluate_accuracy%28net%2C test_iter%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="def softmax%28X%29%3A%0A    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%0A    partition = X_exp.sum%281%2C keepdim=True%29%0A    return X_exp / partition" isIPythonHidden="True" />
<var name="_i40" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A loss%7D%29%0A%0A%23 Optional%0Awandb.watch%28model%29" isIPythonHidden="True" />
<var name="_i41" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i42" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.model%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i43" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.Module%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i44" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.Module%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%0AX_prob = softmax%28X%29%0AX_prob%2C X_prob.sum%281%29" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="def net%28X%29%3A%0A    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="y = torch.tensor%28%5B0%2C 2%5D%29%0Ay_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%0Ay_hat%5B%5B0%2C 1%5D%2C y%5D" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="def cross_entropy%28y_hat%2C y%29%3A%0A    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%0A    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29%0A%0A%0Across_entropy%28y_hat%2C y%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="def accuracy%28y_hat%2C y%29%3A%0A    %22%22%22%E8%AE%A1%E7%AE%97%E9%A2%84%E6%B5%8B%E6%AD%A3%E7%A1%AE%E7%9A%84%E6%95%B0%E9%87%8F%22%22%22%0A    if len%28y_hat.shape%29 &gt; 1 and y_hat.shape%5B1%5D &gt; 1%3A%0A        y_hat = y_hat.argmax%28axis=1%29  %23Todo argmax%0A    cmp = y_hat.type%28y.dtype%29 == y%0A    return float%28cmp.type%28y.dtype%29.sum%28%29%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import torch%5Cnfrom IPython import display%5Cnfrom d2l import torch as d2l%5Cn%5Cnbatch_size = 256%5Cntrain_iter%2C test_iter = d2l.load_data_fashion_mnist%28batch_size%29%27%2C %27num_inputs = 784%5Cnnum_outputs = 10%5Cn%5CnW = torch.normal%280%2C 0.01%2C size=%28num_inputs%2C num_outputs%29%2C requires_grad=True%29%5Cnb = torch.zeros%28num_outputs%2C requires_grad=True%29%5CnW%2C b%27%2C %27X = torch.tensor%28%5B%5B1.%2C 2.%2C 3.%5D%2C %5B4.%2C 5.%2C 6.%5D%5D%29%5CnX.sum%280%2C keepdim=True%29  %23 %E6%8C%89%E5%88%97%E6%B1%82%E5%92%8C keepdim%E8%A1%A8%E7%A4%BA%E4%BF%9D%E6%8C%81%E4%BA%8C%E7%BB%B4%E7%89%B9%E6%80%A7%2C%E5%90%A6%E5%88%99%E4%BC%9A%E9%99%8D%E7%BB%B4%5CnX.sum%281%2C keepdim=True%29  %23 %E6%8C%89%E8%A1%8C%E6%B1%82%E5%92%8C%27%2C %27def softmax%28X%29%3A%5Cn    X_exp = torch.exp%28X%29  %23 %E5%AF%B9%E8%BE%93%E5%85%A5%E7%9A%84X%E8%BF%9B%E8%A1%8Ce%5EX%E6%93%8D%E4%BD%9C%5Cn    partition = X_exp.sum%281%2C keepdim=True%29%5Cn    return X_exp / partition%27%2C %27X = torch.normal%280%2C 1%2C size=%282%2C 5%29%29%5CnX_prob = softmax%28X%29%5CnX_prob%2C X_prob.sum%281%29%27%2C %27def net%28X%29%3A%5Cn    return softmax%28torch.matmul%28X.reshape%28%28-1%2C W.shape%5B0%5D%29%29%2C W%29 %2B b%29  %23 %E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%27%2C %27y = torch.tensor%28%5B0%2C 2%5D%29%5Cny_hat = torch.tensor%28%5B%5B0.1%2C 0.3%2C 0.6%5D%2C %5B0.3%2C 0.2%2C 0.5%5D%5D%29%5Cny_hat%5B%5B0%2C 1%5D%2C y%5D%27%2C %27def cross_entropy%28y_hat%2C y%29%3A%5Cn    %22%22%22%E4%BA%A4%E5%8F%89%E7%86%B5%E5%87%BD%E6%95%B0%E4%BD%9C%E4%B8%BA%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%22%22%22%5Cn    return - torch.log%28y_hat%5Brange%28len%28y_hat%29%29%2C y%5D%29..." isContainer="True" shape="45" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%0A%23 Optional%0Aif isinstance%28net%2C torch.nn.model%29%3A%0A    wandb.watch%28net%29" isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="wandb.log%28%7B%22loss%22%3A cross_entropy%28y_hat%2C y%29%7D%29%0A%0A%23 Optional%0Awandb.watch%28net%29" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B2%3A %28tensor%28%5B%5B 0.0051%2C  0.0121%2C  0.0039%2C  ...%2C  0.0017%2C -0.0086%2C  0.0018%5D%2C%0A        %5B-0.0139%2C -0.0049%2C -0.0035%2C  ...%2C  0.0107%2C  0.0123%2C  0.0171%5D%2C%0A        %5B-0.0028%2C  0.0095%2C  0.0142%2C  ...%2C  0.0044%2C -0.0050%2C  0.0019%5D%2C%0A        ...%2C%0A        %5B-0.0357%2C -0.0009%2C  0.0563%2C  ...%2C -0.0312%2C -0.0588%2C  0.0011%5D%2C%0A        %5B-0.0204%2C -0.0051%2C  0.0284%2C  ...%2C -0.0302%2C -0.0283%2C  0.0053%5D%2C%0A        %5B-0.0126%2C -0.0208%2C  0.0203%2C  ...%2C  0.0030%2C  0.0008%2C -0.0031%5D%5D%2C%0A       requires_grad=True%29%2C tensor%28%5B 0.4622%2C -0.5277%2C -0.0835%2C  0.3629%2C -1.2787%2C  2.4422%2C  0.4982%2C -0.0310%2C%0A        -0.5169%2C -1.3276%5D%2C requires_grad=True%29%29%2C 3%3A tensor%28%5B%5B 6.%5D%2C%0A        %5B15.%5D%5D%29%2C 5%3A %28tensor%28%5B%5B0.1834%2C 0.0725%2C 0.6120%2C 0.0436%2C 0.0884%5D%2C%0A        %5B0.1584%2C 0.2179%2C 0.3293%2C 0.2631%2C 0.0314%5D%5D%29%2C tensor%28%5B1.%2C 1.%5D%29%29%2C 7%3A tensor%28%5B0.1000%2C 0.5000%5D%29%2C 8%3A tensor%28%5B2.3026%2C 0.6931%5D%29%2C 10%3A 0.5%2C 13%3A 0.0673%2C 20%3A 0.8367%2C 22%3A %28tensor%28%5B%5B-0.0149%2C  0.0238%2C  0.0117%2C  ...%2C  0.0137%2C  0.0017%2C  0.0020%5D%2C%0A        %5B-0.0026%2C  0.0068%2C -0.0073%2C  ...%2C  0.0215%2C  0.0051%2C -0.0011%5D%2C%0A        ..." isContainer="True" shape="16" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers%5C%5Cpydev%5C%5C_pydevd_bundle%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="accuracy" type="function" qualifier="builtins" value="%3Cfunction accuracy at 0x000002A8C88E74C0&gt;" isContainer="True" />
<var name="b" type="Tensor" qualifier="torch" value="tensor%28%5B 0.2833%2C -0.3060%2C -0.0895%2C  0.1861%2C -0.9157%2C  1.9069%2C  0.4185%2C -0.1018%2C%0A        -0.4301%2C -0.9516%5D%2C requires_grad=True%29" isContainer="True" shape="(10,)" />
<var name="batch_size" type="int" qualifier="builtins" value="256" />
<var name="cross_entropy" type="function" qualifier="builtins" value="%3Cfunction cross_entropy at 0x000002A8CABD3430&gt;" isContainer="True" />
<var name="d2l" type="module" qualifier="builtins" value="%3Cmodule %27d2l.torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cd2l%5C%5Ctorch.py%27&gt;" isContainer="True" />
<var name="display" type="module" qualifier="builtins" value="%3Cmodule %27IPython.display%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5CIPython%5C%5Cdisplay.py%27&gt;" isContainer="True" />
<var name="evaluate_accuracy" type="function" qualifier="builtins" value="%3Cfunction evaluate_accuracy at 0x000002A8CABDCE50&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x000002A8AAE88F40&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="lr" type="float" qualifier="builtins" value="0.1" />
<var name="net" type="function" qualifier="builtins" value="%3Cfunction net at 0x000002A8CABD39D0&gt;" isContainer="True" />
<var name="num_epochs" type="int" qualifier="builtins" value="10" />
<var name="num_inputs" type="int" qualifier="builtins" value="784" />
<var name="num_outputs" type="int" qualifier="builtins" value="10" />
<var name="print_columns" type="function" qualifier="builtins" value="%3Cfunction print_columns at 0x000002A8CDBF99D0&gt;" isContainer="True" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27E%3A%5C%5CPyCharm 2021.3%5C%5Cplugins%5C%5Cpython%5C%5Chelpers-pro%5C%5Cjupyter_debug%5C%5Cpydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x000002A8AAF04A00&gt;" isContainer="True" isIPythonHidden="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x000002A8C883F9D0&gt;" isContainer="True" />
<var name="softmax" type="function" qualifier="builtins" value="%3Cfunction softmax at 0x000002A8CABDCEE0&gt;" isContainer="True" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="test_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CABD58B0&gt;" isContainer="True" shape="40" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Ctorch%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="train_ch3" type="function" qualifier="builtins" value="%3Cfunction train_ch3 at 0x000002A8CABB9DC0&gt;" isContainer="True" />
<var name="train_epoch_ch3" type="function" qualifier="builtins" value="%3Cfunction train_epoch_ch3 at 0x000002A8CABD3280&gt;" isContainer="True" />
<var name="train_iter" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x000002A8CAC0F9D0&gt;" isContainer="True" shape="235" />
<var name="updater" type="function" qualifier="builtins" value="%3Cfunction updater at 0x000002A8C919E430&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27E%3A%5C%5Cana%5C%5Clib%5C%5Csite-packages%5C%5Cwandb%5C%5C__init__.py%27&gt;" isContainer="True" />
<var name="y" type="Tensor" qualifier="torch" value="tensor%28%5B0%2C 2%5D%29" isContainer="True" shape="(2,)" />
<var name="y_hat" type="Tensor" qualifier="torch" value="tensor%28%5B%5B0.1000%2C 0.3000%2C 0.6000%5D%2C%0A        %5B0.3000%2C 0.2000%2C 0.5000%5D%5D%29" isContainer="True" shape="(2, 3)" />
</xml>